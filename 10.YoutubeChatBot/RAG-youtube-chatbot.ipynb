{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29655ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q youtube-transcript-api pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "55ae5a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pytube import YouTube\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f216b5f",
   "metadata": {},
   "source": [
    "### Step 1a - Indexing (Document Ingestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "87f8b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_youtube_id(url):\n",
    "    return YouTube(url=url).video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8635b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input(\"Enter your URL: \")\n",
    "video_id = get_youtube_id(url)\n",
    "try:\n",
    "    transcript_list =YouTubeTranscriptApi.get_transcript(video_id=video_id, languages=['en'])\n",
    "    transcipt = \" \".join(chunk[\"text\"] for chunk in transcript_list)\n",
    "    # print(transcipt)\n",
    "except TranscriptsDisabled:\n",
    "    print(\"No caption avaiable for this video :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "61e68a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5137"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transcipt.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e460ed",
   "metadata": {},
   "source": [
    "### Step 1b - Indexing (Text Splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c57b537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    "    )  \n",
    "chunks = splitter.create_documents([transcipt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a2f7c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "895e3ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d1ccdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_Store = FAISS.from_documents(chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41678649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'fe5b577f-e280-4351-a234-714229e29d72',\n",
       " 1: 'f39fa7da-86b4-436b-b886-fa27a7c28205',\n",
       " 2: 'a66c8856-cd9c-4b80-b601-3fad3f44bb9e',\n",
       " 3: 'c7562881-04fb-46ed-ac4c-8bd298bf9673',\n",
       " 4: '9eb9eabb-c705-464e-8644-0909673ed570',\n",
       " 5: '32b20d78-7f66-4d2d-9833-3dcd299af1a7',\n",
       " 6: '0ef1e805-1c0d-4124-ae58-1b4a762dd14c',\n",
       " 7: '6881b89e-a88f-4373-a4de-2d951a90fde8',\n",
       " 8: '8bdddf46-24b6-4aa0-94f8-92d40f05597c',\n",
       " 9: '70ac44e0-117e-476f-a94b-53deda85961b',\n",
       " 10: '9e6cf75e-c385-4400-a7df-448a73d7422f',\n",
       " 11: '53e5f57b-cce5-4bf6-9f7d-8a14610b0414',\n",
       " 12: '431a36bf-850a-4d5a-a053-a9e04192842b',\n",
       " 13: '5fa6cf6b-5d80-4a8f-b82c-3eff89f5b75d',\n",
       " 14: 'ea33c1d6-0b17-4498-b395-560b18baaf05',\n",
       " 15: 'd91684a0-fb7c-4b80-8bc4-b468d44d4de2',\n",
       " 16: '8c4e0127-8c9e-44c1-bb24-5dd3e791df18',\n",
       " 17: '0c50072b-7db1-4d5a-9fce-571a12676d6a',\n",
       " 18: '9692e447-da80-4b41-847b-1593df6fd22c',\n",
       " 19: '10174699-f7e7-41f7-921a-2758dccf4fc5',\n",
       " 20: 'ccea4e37-1257-4d88-b42f-f212fa15d95a',\n",
       " 21: 'a0cc497b-738e-42f4-a383-e634f247e996',\n",
       " 22: '4221e17e-7a21-4408-85b5-0c09e07b10dc',\n",
       " 23: '2c9596d1-1b56-4aad-9c6e-a47967dde01f',\n",
       " 24: '00295a85-4ec2-4cec-885f-04b0e0c3f8b1',\n",
       " 25: 'ec9d2f94-b01b-4f90-b78f-88b3db95034d',\n",
       " 26: '5040e28b-d2be-4e00-8fa4-9a0ddf5fbd5a',\n",
       " 27: '9a94d819-bc4d-4bf8-80da-b376e6c4e718',\n",
       " 28: '695c1093-2039-4dce-8d5d-506a2c9368e9',\n",
       " 29: '29d671b1-0afe-4932-92f2-11b970773be0',\n",
       " 30: '19876b85-2c32-429e-a052-af6a2b78edf4',\n",
       " 31: '28abeac0-34f8-4225-82f4-fd977ae1a2ec',\n",
       " 32: '14bab3e2-024d-40ca-8400-27d8b2fbac89',\n",
       " 33: '8c3af938-65d0-4cde-a5b2-a8c34bf1ec04',\n",
       " 34: 'd066225e-8bb1-49b4-a03f-12aeb31f0aa4',\n",
       " 35: 'ce59d5a1-8445-4751-8d65-93fe6f85cfea',\n",
       " 36: 'a4c72c17-bc4b-490e-be41-ad2d25119f50'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_Store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51c356ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a4c72c17-bc4b-490e-be41-ad2d25119f50', metadata={}, page_content=\"review by Patreon supporters. A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review. In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.\")]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_Store.get_by_ids(['a4c72c17-bc4b-490e-be41-ad2d25119f50'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa20576",
   "metadata": {},
   "source": [
    "### Step 2 - Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d45017e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vector_Store.as_retriever(search_type=\"similarity\", kwargs={\"k\":4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "154ab2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x17fbdb410>, search_kwargs={})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "21fe857a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='5fa6cf6b-5d80-4a8f-b82c-3eff89f5b75d', metadata={}, page_content=\"as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format. And if you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling kinda arbitrary. First, whatever kind of model you're making, the input has to be formatted as an array of real numbers. This could simply mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor. You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output. For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens. In deep\"),\n",
       " Document(id='fe5b577f-e280-4351-a234-714229e29d72', metadata={}, page_content=\"The initials GPT stand for Generative Pretrained Transformer. So that first word is straightforward enough, these are bots that generate new text. Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training. But the last word, that's the real key piece. A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI. What I want to do with this video and the following chapters is go through a visually-driven explanation for what actually happens inside a transformer. We're going to follow the data that flows through it and go step by step. There are many different kinds of models that you can build using transformers. Some models take in audio and produce a transcript. This sentence comes from a model going the other way around, producing synthetic speech just from\"),\n",
       " Document(id='9e6cf75e-c385-4400-a7df-448a73d7422f', metadata={}, page_content=\"broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformers specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning. At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves. What I mean by that is, let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word given a passage of text, or any other task that seems to require some element of intuition and pattern recognition. We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in\"),\n",
       " Document(id='70ac44e0-117e-476f-a94b-53deda85961b', metadata={}, page_content=\"but at a high level this is the idea. In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around. If you're comfortable with that background knowledge and a little impatient, you could probably feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer. After that, I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point. For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into\")]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver.invoke(\"What is transformer architecture and how it change the world?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174c57d",
   "metadata": {},
   "source": [
    "### Step 3 - Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ae17d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e4a14714",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a helpful assistant.\n",
    "    Answer ONLY from provide transcript context.\n",
    "    If the context is insufficient, Just say don't know\n",
    "    \n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\",\"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a083cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What actually pre-training mean in transformers?\"\n",
    "retrieve_docs = retriver.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a27ed9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieve_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a2c73fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = prompt.invoke({'context': context_text, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d062046",
   "metadata": {},
   "source": [
    "### Step 5 - Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41d09088",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm.invoke(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d3c25ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine-tune it on specific tasks with additional training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8910250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser  = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "32349100",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser \n",
    "result = chain.invoke({\"context\": context_text, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e56676bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pretrained refers to how the model went through a process of learning from a '\n",
      " \"massive amount of data, and the prefix insinuates that there's more room to \"\n",
      " 'fine-tune it on specific tasks with additional training.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d9c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
